{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact product matching over hackathon_dataset.json\n",
    "\n",
    "* The premisse of this analysis is to match as many products with a very high confidence level in Python using numerical and text methodologies to achive the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzfile = 'dataset.jsonl.gz'\n",
    "jsonfile = 'hackathon_dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip\n",
    "import gzip\n",
    "import shutil\n",
    "with gzip.open(gzfile, 'rb') as gzf:\n",
    "    with open(jsonfile, 'wb') as jsonf:\n",
    "        shutil.copyfileobj(gzf, jsonf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(jsonfile, lines = True)\n",
    "print(df.columns)\n",
    "df.head(10)\n",
    "#df.to_csv('jsonfile.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked the data ouside Python to get the feeling of what the data looks like.\n",
    "\n",
    "On first notice, there are some fields that can be considered for product matching, such as PRODUCT_ID and NAME\n",
    "On a closer inspection, each SHOP brings different (scrapped) data and will need some work to normalize.\n",
    "\n",
    "e.g. \n",
    "* Bol doesn't have SPECIFICATIONS nor CONTENTS but similar products have the same product_id. \n",
    "* Planit has SPECIFICATIONS that are somewhat easy to parse (not directly but with some regex it's easy to create a viable json structure)\n",
    "* Praxis has the same SPECIFICATIONS structure as Planit so can apply the same methods and Planit. Doesn't contain BRAND nor CONTENTS.\n",
    "* Hubo has a different SPECIFICATIONS structure that cannot apply the same method as Planit but can be cleaned and structured. CONTENTS only covers ~21% of the Hubo set. May help a little bit but not much.\n",
    "* Hornbach appears to have same Praxis structure on SPECIFICATIONS but has some minor character encoding errors that should be easy to fix.\n",
    "\n",
    "Considering that each shop has data with different characteristics, I decided to divide each shop as its own subset and work on the matching. Once that done for each shop, then I would cross the data between shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('DALTIX_ID')\n",
    "\n",
    "for i in df.SHOP.unique():\n",
    "    globals()['df_'+str(i)] = df[df['SHOP']==i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process applied on each shop. Due to limited time, was only able to do with BOL.\n",
    "\n",
    "Bol dataset\n",
    "* Most of the times when Product_ID is the same, the product is the same but has some exceptions, specially for color. Therefore it sould be taken in account also the name of the product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dups = df_bol[df_bol[['PRODUCT_ID', 'NAME']].duplicated(keep = False) == True]\n",
    "duplicates_Bol = pd.DataFrame(columns = ['daltix_id_1','daltix_id_2'])\n",
    "for i in Dups.index:\n",
    "    tst = Dups.loc[(Dups[['PRODUCT_ID', 'NAME']]  == Dups.loc[i][['PRODUCT_ID', 'NAME']]) & (Dups.index != i) ]\n",
    "    sub = pd.DataFrame(columns = ['daltix_id_1','daltix_id_2'])\n",
    "    sub['daltix_id_2'] =  tst['DALTIX_ID']\n",
    "    sub['daltix_id_1'] =  Dups['DALTIX_ID'][i]\n",
    "    duplicates_Bol = pd.concat([duplicates_Bol, sub])\n",
    "    \n",
    "#remove mirror pairs\n",
    "duplicates_Bol = duplicates_Bol.loc[pd.DataFrame(np.sort(duplicates_Bol[['daltix_id_1','daltix_id_2']],1),index=duplicates_Bol.index).drop_duplicates(keep='first').index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The previous code didn't take in account for mirror pairs, only identical pair, so I had to include the following \n",
    "#code afterwards to remove mirror pairs\n",
    "\n",
    "duplicates_Bol = duplicates_Bol.loc[pd.DataFrame(np.sort(duplicates_Bol[['daltix_id_1','daltix_id_2']],1),index=duplicates_Bol.index).drop_duplicates(keep='first').index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there are similar names that are not exact match due to case sensitivity, punctuation or accents. \n",
    "For these cases I went looking for normalized names and compare it with the duplicates found previously.\n",
    "\n",
    "Normalize text: lowercase, strip accents, remove ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NAME_CLEAN'] = df['NAME'].str.lower()\n",
    "df['NAME_CLEAN'] = df['NAME_CLEAN'].str.replace('[^\\w\\s]','')\n",
    "df['NAME_CLEAN'] = df['NAME_CLEAN'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dups = df_bol[df_bol.duplicated(subset = ['NAME_CLEAN'], keep = False) == True]\n",
    "duplicates_Bol2 = pd.DataFrame(columns = ['daltix_id_1','daltix_id_2'])\n",
    "for i in Dups.index:\n",
    "    tst = Dups.loc[(Dups['NAME_CLEAN']  == Dups.loc[i]['NAME_CLEAN']) & (Dups['PRODUCT_ID']  != Dups.loc[i]['PRODUCT_ID']) &(Dups.index != i) ]\n",
    "    sub = pd.DataFrame(columns = ['daltix_id_1','daltix_id_2'])\n",
    "    sub['daltix_id_2'] =  tst['DALTIX_ID']\n",
    "    sub['daltix_id_1'] =  Dups['DALTIX_ID'][i]\n",
    "    duplicates_Bol2 = pd.concat([duplicates_Bol2, sub])\n",
    "duplicates_Bol2 = duplicates_Bol2.loc[pd.DataFrame(np.sort(duplicates_Bol2[['daltix_id_1','daltix_id_2']],1),index=duplicates_Bol2.index).drop_duplicates(keep='first').index]\n",
    "\n",
    "duplicates_Bol3 = pd.concat([duplicates_Bol2, duplicates_Bol]).drop_duplicates(keep = False)\n",
    "duplicates_Bol_consolidated = duplicates_Bol3.loc[pd.DataFrame(np.sort(duplicates_Bol3[['daltix_id_1','daltix_id_2']],1),index=duplicates_Bol3.index).drop_duplicates(keep='first').index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that sometimes there are products that are the same but the word order is not the same. To take that in account decided to try fuzzy matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for similarity using fuzzywuzzy\n",
    "https://github.com/seatgeek/fuzzywuzzy\n",
    "\n",
    "Used function token_sort_ratio which allows to calculate the sentence similarity (in this case Levenshtein Distance) without considering word order.\n",
    "Not tested was the function token_set_ratio that ignores duplicate words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NonDups = df_bol[df_bol.duplicated(subset = ['PRODUCT_ID', 'NAME'], keep = False) == False]\n",
    "similarityMatrix = pd.DataFrame(columns = ['daltix_id_1', 'daltix_id_2', 'similarity'])\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "for i in NonDups['DALTIX_ID']:\n",
    "    for j in Dups['DALTIX_ID']:\n",
    "        tmp1 = fuzz.token_sort_ratio(NonDups['NAME'][NonDups['DALTIX_ID']==i],NonDups['NAME'][NonDups['DALTIX_ID']==j])       \n",
    "        similarityMatrix = similarityMatrix.append({'daltix_id_1':i, 'daltix_id_2':j, 'similarity': tmp1 }, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very computer intensive process. \n",
    "* Another approach is use parallelization in chunks to accelerate the process.\n",
    "* Tried to test it out but unfortunately didn't have time to debug it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import concurrent.futures\n",
    "#import multiprocessing\n",
    "#num_processes = multiprocessing.cpu_count()\n",
    "# \n",
    "#NonDups = df_bol[df_bol.duplicated(subset = ['PRODUCT_ID', 'NAME'], keep = False) == False]\n",
    "#SimilarityTable = pd.DataFrame(columns = ['daltix_id_1', 'daltix_id_2', 'similarity'])\n",
    "# \n",
    "#for i in NonDups['DALTIX_ID']:\n",
    "#    sub = pd.DataFrame(columns = ['daltix_id_1','daltix_id_2'])\n",
    "#    sub['daltix_id_2'] =  Dups['DALTIX_ID']\n",
    "#    sub['daltix_id_1'] =  i\n",
    "#    SimilarityTable = pd.concat([SimilarityTable, sub])\n",
    "\n",
    "# Define a function on the numbers\n",
    "#from fuzzywuzzy import fuzz\n",
    "#def sim(id1, id2):\n",
    "#    return fuzz.token_sort_ratio(id1,id2)\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "#    with concurrent.futures.ProcessPoolExecutor(num_processes) as pool:\n",
    "#        SimilarityTable['similarity'] = list(pool.map(sim, SimilarityTable['daltix_id_1'], SimilarityTable['daltix_id_2'], chunksize=1000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching\n",
    "\n",
    "Bol:\n",
    "- Matching using ProductId, Name\n",
    "- Matching using Clean Name\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "Planit\n",
    "- No same ProductID\n",
    "- Matching using Name / CleanName (May need to go indepth to check is specs are really the same)\n",
    "\n",
    "Praxis:\n",
    "- No same ProductID\n",
    "- Matching using Name / CleanName (May need to go indepth to check is specs are really the same)\n",
    "\n",
    "Hornbach:\n",
    "- Name / CleanName Only (May need to go indepth to check is specs are really the same)\n",
    "\n",
    "Hubo:\n",
    "- Name / CleanName Only (May need to go indepth to check is specs are really the same)\n",
    "\n",
    "\n",
    "* While analysing the data, when crossing inter-shop information it was interesting to find that Planit and Praxis use similar productIDs, however NAMES is not exact match. This is where cleaning and fuzzy matching at high confidence level may come handy.\n",
    "\n",
    "For a more indepth analysis and matching both SPECIFICATION and CONTENT should be considered. An easy approach was to use regex to extract some key features. \n",
    "e.g. \n",
    "`re.search(\"(?<='Breedte':\\s)'\\\"(.*?)\\\"',\", str(df['SPECIFICATIONS']i), re.IGNORECASE)`\n",
    "\n",
    "\n",
    "### Future work\n",
    "\n",
    "#### Text / NLP\n",
    "- Products can be divided into diferent types/categories that have similar features (such as weight, length, capacity, etc.). It would be interestig to separate each product in diferent categories using some multi-class categorization algorithm over the DESCRIPTION. This would facilitate further analysis.\n",
    "To do this, further enhancement over the normalization of the corpus is needed. Since it's categorization, we won't need to maintain semantical meaning on the corpus, sowe can not only apply lowercase and remove punctuation but also apply stopwords on td-idf basis and stemming or lemmatization (Frog perhaps? https://languagemachines.github.io/frog/)\n",
    "\n",
    "- Another aproach for products that do not have specs nor contents as a mean to extract structured data, it is to train a NER model to extract from the DESCRIPTION features such as: Brand, size, capacity, color, etc.\n",
    "\n",
    "e.g.\n",
    "{\n",
    "\t\"text\": \"Zomerkleed ovaal 180 microns voor een zwembad met een afmeting van 1000 x 550 cm. Het kleed drijft op het water en is dus iets kleiner dan het zwembad zelf. Het draagt bij aan het minder vuil worden van uw zwembadwater wat een gunstige invloed heeft ophet gebruik van zwembadchemie. Tevens houdt de structuur van het noppenfolie de temperatuur van het zwemwater beter vast, omdat verdamping wordt verminderd, terwijl de warmte van de zon wordt doorgegeven. Afmeting: 995 x 545 cm.\",\n",
    "\n",
    "\t\"tags\": [{\n",
    "\t\t\t\"tag\": \"product\",\n",
    "\t\t\t\"tag_beggining\": 1,\n",
    "\t\t\t\"tag_end\": 10,\n",
    "\t\t\t\"tag_text\": \"Zomerkleed\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"tag\": \"shape\",\n",
    "\t\t\t\"tag_beggining\": 12,\n",
    "\t\t\t\"tag_end\": 17,\n",
    "\t\t\t\"tag_text\": \"ovaal\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"tag\": \"size\",\n",
    "\t\t\t\"tag_beggining\": 468,\n",
    "\t\t\t\"tag_end\": 480,\n",
    "\t\t\t\"tag_text\": \"995 x 545 cm\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "\n",
    "\n",
    "#### Image\n",
    "\n",
    "- Scrapping the product image from the website may also help when the similarity distance is very high, specially for iter-shop products. \n",
    "\n",
    "- Many of the images that appear in the websites use the same product photography. Perhaps using image comparison methodologies may increase the accuracy of the match. Somethin like this OpenCV's Feature Matching and some Homography methodology to take in account rotation/skewness\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
